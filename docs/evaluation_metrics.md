# Code Review Assistant - Evaluation Metrics Sheet

## Test Configuration

| Parameter | Value |
|-----------|-------|
| Test Date | 2026-01-27 |
| Ground Truth Issues | 39 |
| Test Repository | evaluation/seed_repo |
| LLM Model | llama-3.1-8b-instant |
| LLM Temperature | 0 (deterministic) |

---

## Overall Metrics

| Metric | Formula | Value |
|--------|---------|-------|
| **True Positives (TP)** | Correctly detected issues | 27 |
| **False Positives (FP)** | Incorrectly flagged as issues | 19 |
| **False Negatives (FN)** | Missed actual issues | 13 |
| **Precision** | TP / (TP + FP) | **58.70%** |
| **Recall** | TP / (TP + FN) | **67.50%** |
| **F1-Score** | 2 × (P × R) / (P + R) | **62.79%** |

---

## Metrics by Category

| Category | TP | FP | FN | Precision | Recall | F1-Score |
|----------|----|----|-----|-----------|--------|----------|
| **Security** | 8 | 1 | 2 | 88.89% | 80.00% | **84.21%** |
| **Complexity** | 4 | 2 | 1 | 66.67% | 80.00% | **72.73%** |
| **Readability** | 10 | 11 | 3 | 47.62% | 76.92% | **58.82%** |
| **Correctness** | 5 | 5 | 7 | 50.00% | 41.67% | **45.45%** |

---

## Metrics by Severity

| Severity | TP | FP | FN | Precision | Recall | F1-Score |
|----------|----|----|-----|-----------|--------|----------|
| **High** | 10 | 7 | 2 | 58.82% | 83.33% | **68.97%** |
| **Medium** | 6 | 6 | 6 | 50.00% | 50.00% | **50.00%** |
| **Low** | 11 | 6 | 5 | 64.71% | 68.75% | **66.67%** |

---

## Metrics by Language

| Language | Ground Truth | Detected | TP | Precision | Recall |
|----------|--------------|----------|----|-----------| -------|
| Python | 23 | 28 | 17 | 60.71% | 73.91% |
| JavaScript | 16 | 18 | 10 | 55.56% | 62.50% |

---

## Detection Source Analysis

| Source | Issues Found | Matched (TP) | Match Rate |
|--------|--------------|--------------|------------|
| Static Analysis | 18 | 12 | 66.67% |
| LLM Review | 28 | 15 | 53.57% |

---

## Key Findings

### Strengths
1. **Security Detection** - Highest F1-score at 84.21%
   - Successfully detects SQL injection, eval() usage, hardcoded credentials
   - Low false positive rate for security issues

2. **High Severity Issues** - 83.33% recall
   - Critical issues are rarely missed
   - Good prioritization of important problems

3. **Complexity Analysis** - 72.73% F1-score
   - Effective at identifying cyclomatic complexity
   - Good detection of nested callbacks

### Areas for Improvement
1. **Correctness Category** - 45.45% F1-score
   - Missing bare except, mutable defaults, global variable issues
   - Consider adding more static analysis rules

2. **Readability Category** - 47.62% precision
   - High false positive rate
   - Over-flagging stylistic issues

3. **Medium Severity Issues** - 50% F1-score
   - Balanced but could be improved
   - Better calibration needed for severity classification

---

## Recommendations

1. **Add more Python-specific rules** for:
   - Mutable default arguments
   - Global variable modification
   - Type comparison with isinstance()

2. **Improve LLM prompt** to reduce false positives for:
   - Stylistic issues
   - Context-dependent patterns

3. **Add severity calibration** based on:
   - Issue frequency in codebase
   - Potential impact assessment

---

## Test Files Summary

| File | Ground Truth Issues | Detected | Matched |
|------|---------------------|----------|---------|
| src/auth.py | 9 | 10 | 7 |
| src/database.py | 7 | 9 | 5 |
| src/utils.py | 8 | 10 | 5 |
| src/api.js | 9 | 10 | 6 |
| src/components.jsx | 7 | 7 | 4 |

---

*Generated by Code Review Assistant Evaluation System*
*Report Date: 2026-01-27*
